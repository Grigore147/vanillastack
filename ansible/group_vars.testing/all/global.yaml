---
commercial:
  registry:
    url: harbor.cloudical.net
    username: 'gitlab-testing-stage'
    key: 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE2MDI1MzYzNTksImlzcyI6ImhhcmJvci10b2tlbi1kZWZhdWx0SXNzdWVyIiwiaWQiOjEwNzIsInBpZCI6MywiYWNjZXNzIjpbeyJSZXNvdXJjZSI6Ii9wcm9qZWN0LzMvcmVwb3NpdG9yeSIsIkFjdGlvbiI6InB1bGwiLCJFZmZlY3QiOiIifSx7IlJlc291cmNlIjoiL3Byb2plY3QvMy9oZWxtLWNoYXJ0IiwiQWN0aW9uIjoicmVhZCIsIkVmZmVjdCI6IiJ9XX0.f36zYoZF3anzW4lkgXO5CbP8IenH8rwKlM8QRRGnd6kqkwEPndIgUMjqJLibtph1OcBOj6jQiIg8Uw8W30dgxvbAStZnZpOi7NSN8NvAr9n5P1alVLCAxMCqcQRIWxwfXquwcWDbCaoTUZUTTckf5FqQn1ZvtUOnaktW69qTF7HuYicVqkgqABkRqUXQYRaIX_nLo46xGhu5-luNwK66LgafhRwt_XshFMiQQwwxdPRuxWXzwyQdz5dzvwKsskx-kKaUmRzbl4Dxw50hX4UnwMdrTr9n_EsOGsm0_0cTJaFcuvLqcxPlLMU4OEr6VfbT3JC2k8yK7Js4SWkfoWMQRFNH88-fNJEeU6PJ62nydNJLKVK9LHpWWp70K0GHj9b2g6xf5VKSE3_Hie2g20BYKO91wbN-J952sgPNG7xx1YgKHXsfxHkUVwRL4jaQcprXgWmgC0MtujXg4eX08bUY3nefJdQ1Nv8g77IddfTp6XeVEjLgou0AL6n-N9fyjtR4p3SZOxdP6L7UKwxJFMRmtfwCCUq2CbVm84-MY4fODh-WfmryENXynIO0X6bM2xfOLvbktwYKpYYeehNpbdauc96ui6WZckIp5gUmZxgO0NhlAZzhLBIKJ36gIaoGNpIJVLuCtkVPoEtJRdZNBofhneFSSCTAL1eHTkN8DKOvVIM'
repo:
  registry: harbor.vanillastack.io
  url:  https://repo.vanillastack.cloudical.net
ingress:
  namespace: nginx-ingress
  enabled: true
kubernetes:
  dashboard:
    coreDomain: "k8sboard.{{ clusterTLDomain }}"
  clusterName: kube
  version: "1.19"
  crioVersion: "1.19"
  helm_version: "v3.3.1"
  pod_cidr: "172.16.0.0/12"
  service_cidr: "172.31.0.0/16"
  init_opts: ""
  kubeadm_opts: ""
  kubeadmin_config: /etc/kubernetes/admin.conf
  cilium:
    version: v1.8.6
cloudfoundry:
  storageclass: rook-ceph-block
  coreDomain: "cf.{{ clusterTLDomain }}"
stratos:
  adminpassword: 4JOA4l8DK7Sr
  coreDomain: "stratos.{{ clusterTLDomain }}"
guacamole:
  imageTag: 202009100849
  namespace: guacamole
  coreDomain: "guacamole.{{ clusterTLDomain }}"
  postgres:
    maxConnections: 400
    backup:
      enabled: false
      cronjob: "30 */2 * * *"
    version: "12"
    replicas: "2"
    size: "20Gi"
postgresOperator:
  chartName: "postgres-operator"
  chartVersion: "1.5.0"
  namespace: "postgres-operator"
  backup:
    enabled: false
    s3:
      accessKey: nul
      secretKey: nul
      regionName: nul
      endpoint: nul
      bucketName: nul
    cron: "30 00 * * *"
redis:
  namespace: "testing"
  name: "redis-cluster"
  global:
    imagetag: "2.0"
    password: "Opstree@1235"
  operator:
    imagetag: "v0.2"
  exporter:
    imagetag: "1.0"    
harbor:
  chartVersion: 1.5.1
  namespace: harbor
  ingress:
    coreDomain: "harbor.{{ clusterTLDomain }}"
    notaryDomain: "notary.{{ clusterTLDomain }}"
  tls:
    enabled: true
  portal:
    replicas: 1
  core:
    replicas: 1
  jobservice:
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 15Gi
  registry:
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 10Gi
  chartmuseum:
    enabled: true
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 5Gi
  clair:
    enabled: true
    replicas: 1
  trivy:
    enabled: true
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 5Gi
  notary:
    enabled: true
    replicas: 1
  auth:
    password: 02ZVPokFHXPHfSfkfQGCWIfmJ
    # secret lenght must be 16
    secret: p1zX2AazSCwtUfmv
  redis:
    chartVersion: 10.8.1
    password: zP7GPoCnFNa6XiVnsLVJ3jJgw
    persistence:
      storageClass: "rook-ceph-block"
      size: 8Gi
  postgres:
    size: "20Gi"
    replicas: 2
    version: "12"
    maxConnections: 400
    backup:
      enabled: false
      cronjob: "30 */2 * * *"
efkstack:
  imageTag: "7.9.1"
  kibana:
    user: kibana_test
    password: secretlysecretpassword
    replicas: 2
    system_user: "{{ ansible_user }}"
    system_group: "{{ ansible_user }}"
    coreDomain: "kibana.{{ clusterTLDomain }}"
    resources:
      requests:
        cpu: "200m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  es:
    replicas: 3
    storage:
      class_name: "rook-ceph-block"
      size: "30Gi"
    resources:
      requests:
        cpu: "200m"
        memory: "2Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  logaggregator:
    # Choice of filebeat and fluentd
    name: filebeat
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "200Mi"
monitoring:
  admin_user: admin
  admin_password: prompassword
  replicas: 2
  ingress:
    enabled: true
    dns: "prometheus.{{ clusterTLDomain }}"
  storage:
    classname: "rook-ceph-block"
    size: 30Gi
  resources:
    limits:
      cpu: 2500m
      memory: 2048Mi
    requests:
      cpu: 200m
      memory: 512Mi
  kubeApiServer:
    enabled: true
  kubelet:
    enabled: true
  kubeControllerManager:
    enabled: true
  coreDns:
    enabled: true
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: true
  kubeScheduler:
    enabled: true
  kubeProxy:
    enabled: true
  kubeStateMetrics:
    enabled: true
alertmanager:
  enabled: true
  admin_user: admin
  admin_password: ampassword
  replicas: 2
  ingress:
    enabled: true
    dns: "alertmanager.{{ clusterTLDomain }}"
  storage:
    classname: "rook-ceph-block"
    size: 10Gi
  resources:
    limits:
      cpu: 2500m
      memory: 2048Mi
    requests:
      cpu: 200m
      memory: 512Mi
  config: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    receivers:
    - name: 'null'
grafana:
  enabled: true
  admin_user: admin
  admin_password: grafanapassword
  ingress:
    enabled: true
    dns: "grafana.{{ clusterTLDomain }}"
kubevirt:
  namespace: kubevirt
  version: v0.33.0
moodle:
  username: admin
  namespace: moodle
  storage:
    classname: "rook-ceph-block"
    size: 8Gi
  mariadb:
    persistence: true
    classname: "rook-ceph-block"
    size: 8Gi
  ingress:
    enabled: true
    dns: "moodle.{{ clusterTLDomain }}"
keycloak:
  namespace: keycloak
  externalDatabase: false
  postgres:
    size: "20Gi"
    replicas: 2
    version: "12"
    maxConnections: 400
    backup:
      enabled: false
      cronjob: "30 */2 * * *"
  ingress:
    enabled: true
    publicDomain: "keycloak.{{ clusterTLDomain }}"
  realm:
    name: "{{ clusterTLDomain }}"
    id: "{{ clusterTLDomain }}"
